{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1168,
     "status": "ok",
     "timestamp": 1731992687204,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "Dh-HVly3UbmJ",
    "outputId": "e180ca28-23e3-4b15-b174-edf1ca8af950"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Google Drive + working directory path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PATH = '/content/drive/MyDrive/Vision24/'\n",
    "VERSION = 'squeeze_and_excitation'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731992687204,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "E06Oj02_U9YW",
    "outputId": "2b4640f9-b0c3-453f-bd74-1cd62e6b331f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.isdir(PATH))\n",
    "assert os.path.isdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1731992687204,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "VcUeV6IWV_XH",
    "outputId": "ab35c4c5-609d-4774-a861-c51a763f39da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = os.path.join(PATH, VERSION)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "print(os.path.isdir(CHECKPOINT_PATH))\n",
    "assert os.path.isdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 166,
     "status": "ok",
     "timestamp": 1731992687367,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "ZsVjFoYbpMLY",
    "outputId": "32171432-6f40-4486-ba85-77a70aee246d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 19 05:04:47 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   39C    P0              66W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# verify >16GB of RAM\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8376,
     "status": "ok",
     "timestamp": 1731992695741,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "-H3kp0n20z4P"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install zetascale\n",
    "!pip install swarms\n",
    "!pip install torchinfo\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn, Tensor\n",
    "from zeta.nn import SSM\n",
    "from einops.layers.torch import Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731992695742,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "TdMgDLMzkq0x"
   },
   "outputs": [],
   "source": [
    "# Squeeze and Excitation block\n",
    "from torch.nn import AdaptiveAvgPool1d, Linear, Sigmoid, ReLU\n",
    "\n",
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, channel, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.avg_pool = nn.AdaptiveAvgPool1d(1)  # Global average pooling over sequence length\n",
    "        self.fc1 = nn.Linear(channel, channel // reduction, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(channel // reduction, channel, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, s, d = x.size()  # Input shape [B, S, D]\n",
    "        x = x.transpose(1, 2)  # New shape: [B, D, S]\n",
    "\n",
    "        y = self.avg_pool(x).view(b, d)  # Result is [B, D]\n",
    "\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y)\n",
    "\n",
    "        y = y.view(b, d, 1) # Reshape back to [B, D, 1]\n",
    "        y = y.expand(-1, -1, s)  # Broadcast to [B, D, S]\n",
    "\n",
    "        out = x * y  # Element-wise scaling. Shape: [B, D, S]\n",
    "        out = out.transpose(1,2) # Transpose back to original shape: [B, S, D]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731992695742,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "b-qEXTAD0rU8"
   },
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "def output_head(dim: int, num_classes: int):\n",
    "    \"\"\"\n",
    "    Creates a head for the output layer of a model.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the head.\n",
    "        num_classes (int): The number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: The output head module.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        Reduce(\"b s d -> b d\", \"mean\"),\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, num_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "class VisionEncoderMambaBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dt_rank: int,\n",
    "        dim_inner: int,\n",
    "        d_state: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "        self.forward_conv1d = nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1)\n",
    "        self.backward_conv1d = nn.Conv1d(in_channels=dim, out_channels=dim, kernel_size=1)\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.ssm = SSM(dim, dt_rank, dim_inner, d_state)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim) # projection layer\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "        self.se = SEBlock(channel=dim, reduction=4) # Squeeze and Excite Block\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, s, d = x.shape\n",
    "\n",
    "        skip = x            # skip connection\n",
    "        x = self.norm(x)\n",
    "        z = self.silu(self.proj(x))   # project --> activation for gating\n",
    "        x = self.proj(x)\n",
    "\n",
    "        x1 = self.process_direction(x, self.forward_conv1d, self.ssm,)\n",
    "        x2 = self.process_direction(x, self.backward_conv1d, self.ssm,)\n",
    "\n",
    "        x1 *= z\n",
    "        x2 *= z\n",
    "\n",
    "        x = x1 + x2\n",
    "        x = self.se(x)  # SE block to recalibrate the main branch\n",
    "        return x + skip\n",
    "\n",
    "    def process_direction(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        conv1d: nn.Conv1d,\n",
    "        ssm: SSM,\n",
    "    ):\n",
    "        x = rearrange(x, \"b s d -> b d s\")\n",
    "        x = self.softplus(conv1d(x))\n",
    "        x = rearrange(x, \"b d s -> b s d\")\n",
    "        x = ssm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Vim(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dt_rank: int = 32,\n",
    "        dim_inner: int = None,\n",
    "        d_state: int = None,\n",
    "        num_classes: int = None,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        channels: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        depth: int = 12,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        self.dropout = dropout\n",
    "        self.depth = depth\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_height,\n",
    "            ),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.layers = nn.ModuleList()        # encoder layers\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                VisionEncoderMambaBlock(\n",
    "                    dim=dim,\n",
    "                    dt_rank=dt_rank,\n",
    "                    dim_inner=dim_inner,\n",
    "                    d_state=d_state,\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            )\n",
    "        self.output_head = output_head(dim, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, \"() n d -> b n d\", b=b)\n",
    "        # x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        # x = reduce(x, \"b s d -> b d\", \"mean\")\n",
    "        return self.output_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1780,
     "status": "ok",
     "timestamp": 1731992697517,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "OYfhf42jJcxS",
    "outputId": "51af33e0-5b91-4891-fbce-f4618d9a8661"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import repeat\n",
    "\n",
    "model = Vim(dim=96,\n",
    "            dt_rank=16,\n",
    "            dim_inner=96,\n",
    "            d_state=96,\n",
    "            num_classes=10,\n",
    "            image_size=32,\n",
    "            patch_size=4,\n",
    "            channels=3,\n",
    "            dropout=0.1,\n",
    "            depth=10,)\n",
    "\n",
    "batch_size = 256\n",
    "epochs = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "training_data = torchvision.datasets.CIFAR10(root = './dataa',train=True,download=True,transform=transform)\n",
    "testing_data = torchvision.datasets.CIFAR10(root = './data',train=False,download=True,transform=transform)\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testing_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731992697517,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "IfWldubxTfy_",
    "outputId": "194c71e7-1d77-4281-866e-edb275a77c46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  100\n",
      "Training data:  196\n",
      "Total steps:  19600\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('epochs: ', epochs)\n",
    "print('Training data: ', len(training_loader))\n",
    "print('Total steps: ', epochs*len(training_loader))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731992697517,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "tO2AQTt3gcE7",
    "outputId": "331d86a5-8a5b-484b-b959-7ad514ade7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1731992697.3588414\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "_start = time.time()\n",
    "print(_start)\n",
    "\n",
    "_training_losses = []\n",
    "__n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2850600,
     "status": "ok",
     "timestamp": 1732008570474,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "x6wPqp21LIyF",
    "outputId": "c95a1be6-f2e7-435f-87ce-2ac1a0a2018e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [1/196], Loss: 0.0250\n",
      "Epoch [1/100], Step [101/196], Loss: 1.9431\n",
      "Epoch 1 finished. Loss: 1.8478009068236059\n",
      "Epoch [2/100], Step [1/196], Loss: 0.0159\n",
      "Epoch [2/100], Step [101/196], Loss: 1.7155\n",
      "Epoch 2 finished. Loss: 1.6897904945879567\n",
      "Epoch [3/100], Step [1/196], Loss: 0.0180\n",
      "Epoch [3/100], Step [101/196], Loss: 1.7042\n",
      "Epoch 3 finished. Loss: 1.6557764927951657\n",
      "Epoch [4/100], Step [1/196], Loss: 0.0150\n",
      "Epoch [4/100], Step [101/196], Loss: 1.5793\n",
      "Epoch 4 finished. Loss: 1.5374035768362941\n",
      "Epoch [5/100], Step [1/196], Loss: 0.0146\n",
      "Epoch [5/100], Step [101/196], Loss: 1.4248\n",
      "Epoch 5 finished. Loss: 1.4144263066807572\n",
      "Epoch [6/100], Step [1/196], Loss: 0.0144\n",
      "Epoch [6/100], Step [101/196], Loss: 1.3937\n",
      "Epoch 6 finished. Loss: 1.396831759992911\n",
      "Epoch [7/100], Step [1/196], Loss: 0.0131\n",
      "Epoch [7/100], Step [101/196], Loss: 1.3980\n",
      "Epoch 7 finished. Loss: 1.3835279333348176\n",
      "Epoch [8/100], Step [1/196], Loss: 0.0137\n",
      "Epoch [8/100], Step [101/196], Loss: 1.3798\n",
      "Epoch 8 finished. Loss: 1.40679148569399\n",
      "Epoch [9/100], Step [1/196], Loss: 0.0169\n",
      "Epoch [9/100], Step [101/196], Loss: 1.4716\n",
      "Epoch 9 finished. Loss: 1.4062934992264728\n",
      "Epoch [10/100], Step [1/196], Loss: 0.0122\n",
      "Epoch [10/100], Step [101/196], Loss: 1.2898\n",
      "Epoch 10 finished. Loss: 1.2906008788517542\n",
      "Epoch [11/100], Step [1/196], Loss: 0.0122\n",
      "Epoch [11/100], Step [101/196], Loss: 1.5737\n",
      "Epoch 11 finished. Loss: 1.7398501707583058\n",
      "Epoch [12/100], Step [1/196], Loss: 0.0173\n",
      "Epoch [12/100], Step [101/196], Loss: 1.7551\n",
      "Epoch 12 finished. Loss: 1.7197676659846792\n",
      "Epoch [13/100], Step [1/196], Loss: 0.0161\n",
      "Epoch [13/100], Step [101/196], Loss: 1.6231\n",
      "Epoch 13 finished. Loss: 1.6031680094952485\n",
      "Epoch [14/100], Step [1/196], Loss: 0.0150\n",
      "Epoch [14/100], Step [101/196], Loss: 1.5476\n",
      "Epoch 14 finished. Loss: 1.5348398770604814\n",
      "Epoch [15/100], Step [1/196], Loss: 0.0144\n",
      "Epoch [15/100], Step [101/196], Loss: 1.4870\n",
      "Epoch 15 finished. Loss: 1.476851041827883\n",
      "Epoch [16/100], Step [1/196], Loss: 0.0138\n",
      "Epoch [16/100], Step [101/196], Loss: 1.4447\n",
      "Epoch 16 finished. Loss: 1.4297596197955462\n",
      "Epoch [17/100], Step [1/196], Loss: 0.0142\n",
      "Epoch [17/100], Step [101/196], Loss: 1.3995\n",
      "Epoch 17 finished. Loss: 1.4021936721947728\n",
      "Epoch [18/100], Step [1/196], Loss: 0.0136\n",
      "Epoch [18/100], Step [101/196], Loss: 1.3498\n",
      "Epoch 18 finished. Loss: 1.3536669879543537\n",
      "Epoch [19/100], Step [1/196], Loss: 0.0126\n",
      "Epoch [19/100], Step [101/196], Loss: 1.3257\n",
      "Epoch 19 finished. Loss: 1.3615890388586083\n",
      "Epoch [20/100], Step [1/196], Loss: 0.0132\n",
      "Epoch [20/100], Step [101/196], Loss: 1.3441\n",
      "Epoch 20 finished. Loss: 1.3269893253336147\n",
      "Epoch [21/100], Step [1/196], Loss: 0.0110\n",
      "Epoch [21/100], Step [101/196], Loss: 1.2874\n",
      "Epoch 21 finished. Loss: 1.397488997298844\n",
      "Epoch [22/100], Step [1/196], Loss: 0.0206\n",
      "Epoch [22/100], Step [101/196], Loss: 1.9563\n",
      "Epoch 22 finished. Loss: 1.887072395913455\n",
      "Epoch [23/100], Step [1/196], Loss: 0.0169\n",
      "Epoch [23/100], Step [101/196], Loss: 1.7046\n",
      "Epoch 23 finished. Loss: 1.6832823710782188\n",
      "Epoch [24/100], Step [1/196], Loss: 0.0158\n",
      "Epoch [24/100], Step [101/196], Loss: 1.6032\n",
      "Epoch 24 finished. Loss: 1.589628406325165\n",
      "Epoch [25/100], Step [1/196], Loss: 0.0157\n",
      "Epoch [25/100], Step [101/196], Loss: 1.5456\n",
      "Epoch 25 finished. Loss: 1.5285133099069401\n",
      "Epoch [26/100], Step [1/196], Loss: 0.0143\n",
      "Epoch [26/100], Step [101/196], Loss: 1.4853\n",
      "Epoch 26 finished. Loss: 1.4824405756531929\n",
      "Epoch [27/100], Step [1/196], Loss: 0.0147\n",
      "Epoch [27/100], Step [101/196], Loss: 1.4602\n",
      "Epoch 27 finished. Loss: 1.4490539997207874\n",
      "Epoch [28/100], Step [1/196], Loss: 0.0144\n",
      "Epoch [28/100], Step [101/196], Loss: 1.4295\n",
      "Epoch 28 finished. Loss: 1.4299506216633076\n",
      "Epoch [29/100], Step [1/196], Loss: 0.0138\n",
      "Epoch [29/100], Step [101/196], Loss: 1.4059\n",
      "Epoch 29 finished. Loss: 1.400342083098937\n",
      "Epoch [30/100], Step [1/196], Loss: 0.0143\n",
      "Epoch [30/100], Step [101/196], Loss: 1.3711\n",
      "Epoch 30 finished. Loss: 1.3744304757945391\n",
      "Epoch [31/100], Step [1/196], Loss: 0.0129\n",
      "Epoch [31/100], Step [101/196], Loss: 1.3625\n",
      "Epoch 31 finished. Loss: 1.3550956601999244\n",
      "Epoch [32/100], Step [1/196], Loss: 0.0144\n",
      "Epoch [32/100], Step [101/196], Loss: 1.3417\n",
      "Epoch 32 finished. Loss: 1.3406080506285842\n",
      "Epoch [33/100], Step [1/196], Loss: 0.0128\n",
      "Epoch [33/100], Step [101/196], Loss: 1.3278\n",
      "Epoch 33 finished. Loss: 1.3273365917254467\n",
      "Epoch [34/100], Step [1/196], Loss: 0.0130\n",
      "Epoch [34/100], Step [101/196], Loss: 1.3089\n",
      "Epoch 34 finished. Loss: 1.3098309715183414\n",
      "Epoch [35/100], Step [1/196], Loss: 0.0121\n",
      "Epoch [35/100], Step [101/196], Loss: 1.3422\n",
      "Epoch 35 finished. Loss: 1.3176835817950112\n",
      "Epoch [36/100], Step [1/196], Loss: 0.0133\n",
      "Epoch [36/100], Step [101/196], Loss: 1.2889\n",
      "Epoch 36 finished. Loss: 1.2901411324131244\n",
      "Epoch [37/100], Step [1/196], Loss: 0.0119\n",
      "Epoch [37/100], Step [101/196], Loss: 1.2680\n",
      "Epoch 37 finished. Loss: 1.264253881512856\n",
      "Epoch [38/100], Step [1/196], Loss: 0.0120\n",
      "Epoch [38/100], Step [101/196], Loss: 1.2577\n",
      "Epoch 38 finished. Loss: 1.253635370001501\n",
      "Epoch [39/100], Step [1/196], Loss: 0.0118\n",
      "Epoch [39/100], Step [101/196], Loss: 1.2390\n",
      "Epoch 39 finished. Loss: 1.241006921140515\n",
      "Epoch [40/100], Step [1/196], Loss: 0.0117\n",
      "Epoch [40/100], Step [101/196], Loss: 1.2339\n",
      "Epoch 40 finished. Loss: 1.2295972957294814\n",
      "Epoch [41/100], Step [1/196], Loss: 0.0122\n",
      "Epoch [41/100], Step [101/196], Loss: 1.2195\n",
      "Epoch 41 finished. Loss: 1.2205050386944596\n",
      "Epoch [42/100], Step [1/196], Loss: 0.0116\n",
      "Epoch [42/100], Step [101/196], Loss: 1.1954\n",
      "Epoch 42 finished. Loss: 1.2083368477772694\n",
      "Epoch [43/100], Step [1/196], Loss: 0.0121\n",
      "Epoch [43/100], Step [101/196], Loss: 1.1915\n",
      "Epoch 43 finished. Loss: 1.1965963706678273\n",
      "Epoch [44/100], Step [1/196], Loss: 0.0113\n",
      "Epoch [44/100], Step [101/196], Loss: 1.1905\n",
      "Epoch 44 finished. Loss: 1.1909066590727593\n",
      "Epoch [45/100], Step [1/196], Loss: 0.0106\n",
      "Epoch [45/100], Step [101/196], Loss: 1.1771\n",
      "Epoch 45 finished. Loss: 1.1844306758471899\n",
      "Epoch [46/100], Step [1/196], Loss: 0.0117\n",
      "Epoch [46/100], Step [101/196], Loss: 1.1716\n",
      "Epoch 46 finished. Loss: 1.1744929092879197\n",
      "Epoch [47/100], Step [1/196], Loss: 0.0111\n",
      "Epoch [47/100], Step [101/196], Loss: 1.1699\n",
      "Epoch 47 finished. Loss: 1.1717018579341927\n",
      "Epoch [48/100], Step [1/196], Loss: 0.0120\n",
      "Epoch [48/100], Step [101/196], Loss: 1.1708\n",
      "Epoch 48 finished. Loss: 1.1661337246091998\n",
      "Epoch [49/100], Step [1/196], Loss: 0.0108\n",
      "Epoch [49/100], Step [101/196], Loss: 1.1443\n",
      "Epoch 49 finished. Loss: 1.1511483207649114\n",
      "Epoch [50/100], Step [1/196], Loss: 0.0114\n",
      "Epoch [50/100], Step [101/196], Loss: 1.1499\n",
      "Epoch 50 finished. Loss: 1.1548917360451756\n",
      "Epoch [51/100], Step [1/196], Loss: 0.0107\n",
      "Epoch [51/100], Step [101/196], Loss: 1.1476\n",
      "Epoch 51 finished. Loss: 1.1497222312859126\n",
      "Epoch [52/100], Step [1/196], Loss: 0.0113\n",
      "Epoch [52/100], Step [101/196], Loss: 1.1489\n",
      "Epoch 52 finished. Loss: 1.1582062083239457\n",
      "Epoch [53/100], Step [1/196], Loss: 0.0115\n",
      "Epoch [53/100], Step [101/196], Loss: 1.1750\n",
      "Epoch 53 finished. Loss: 1.1619536073840395\n",
      "Epoch [54/100], Step [1/196], Loss: 0.0128\n",
      "Epoch [54/100], Step [101/196], Loss: 1.1264\n",
      "Epoch 54 finished. Loss: 1.1328284314700536\n",
      "Epoch [55/100], Step [1/196], Loss: 0.0112\n",
      "Epoch [55/100], Step [101/196], Loss: 1.1166\n",
      "Epoch 55 finished. Loss: 1.1312766199817463\n",
      "Epoch [56/100], Step [1/196], Loss: 0.0098\n",
      "Epoch [56/100], Step [101/196], Loss: 1.6439\n",
      "Epoch 56 finished. Loss: 1.654562771320343\n",
      "Epoch [57/100], Step [1/196], Loss: 0.0154\n",
      "Epoch [57/100], Step [101/196], Loss: 1.5645\n",
      "Epoch 57 finished. Loss: 1.534720202489775\n",
      "Epoch [58/100], Step [1/196], Loss: 0.0153\n",
      "Epoch [58/100], Step [101/196], Loss: 1.4577\n",
      "Epoch 58 finished. Loss: 1.4439233809101337\n",
      "Epoch [59/100], Step [1/196], Loss: 0.0150\n",
      "Epoch [59/100], Step [101/196], Loss: 1.3965\n",
      "Epoch 59 finished. Loss: 1.3877815257529824\n",
      "Epoch [60/100], Step [1/196], Loss: 0.0138\n",
      "Epoch [60/100], Step [101/196], Loss: 1.3476\n",
      "Epoch 60 finished. Loss: 1.3413299662726266\n",
      "Epoch [61/100], Step [1/196], Loss: 0.0128\n",
      "Epoch [61/100], Step [101/196], Loss: 1.3153\n",
      "Epoch 61 finished. Loss: 1.3085189881373425\n",
      "Epoch [62/100], Step [1/196], Loss: 0.0128\n",
      "Epoch [62/100], Step [101/196], Loss: 1.2810\n",
      "Epoch 62 finished. Loss: 1.2766699815283016\n",
      "Epoch [63/100], Step [1/196], Loss: 0.0121\n",
      "Epoch [63/100], Step [101/196], Loss: 1.2619\n",
      "Epoch 63 finished. Loss: 1.2538464604591837\n",
      "Epoch [64/100], Step [1/196], Loss: 0.0118\n",
      "Epoch [64/100], Step [101/196], Loss: 1.2364\n",
      "Epoch 64 finished. Loss: 1.2331023021620147\n",
      "Epoch [65/100], Step [1/196], Loss: 0.0124\n",
      "Epoch [65/100], Step [101/196], Loss: 1.2139\n",
      "Epoch 65 finished. Loss: 1.2148118286716694\n",
      "Epoch [66/100], Step [1/196], Loss: 0.0125\n",
      "Epoch [66/100], Step [101/196], Loss: 1.2007\n",
      "Epoch 66 finished. Loss: 1.1991449186996537\n",
      "Epoch [67/100], Step [1/196], Loss: 0.0122\n",
      "Epoch [67/100], Step [101/196], Loss: 1.1864\n",
      "Epoch 67 finished. Loss: 1.1817172485954908\n",
      "Epoch [68/100], Step [1/196], Loss: 0.0121\n",
      "Epoch [68/100], Step [101/196], Loss: 1.1654\n",
      "Epoch 68 finished. Loss: 1.162934845807601\n",
      "Epoch [69/100], Step [1/196], Loss: 0.0122\n",
      "Epoch [69/100], Step [101/196], Loss: 1.1477\n",
      "Epoch 69 finished. Loss: 1.155627646616527\n",
      "Epoch [70/100], Step [1/196], Loss: 0.0119\n",
      "Epoch [70/100], Step [101/196], Loss: 1.1420\n",
      "Epoch 70 finished. Loss: 1.1475377660624835\n",
      "Epoch [71/100], Step [1/196], Loss: 0.0122\n",
      "Epoch [71/100], Step [101/196], Loss: 1.1412\n",
      "Epoch 71 finished. Loss: 1.135197217975344\n",
      "Epoch [72/100], Step [1/196], Loss: 0.0114\n",
      "Epoch [72/100], Step [101/196], Loss: 1.1299\n",
      "Epoch 72 finished. Loss: 1.128611185112778\n",
      "Epoch [73/100], Step [1/196], Loss: 0.0118\n",
      "Epoch [73/100], Step [101/196], Loss: 1.1100\n",
      "Epoch 73 finished. Loss: 1.1139516666227458\n",
      "Epoch [74/100], Step [1/196], Loss: 0.0127\n",
      "Epoch [74/100], Step [101/196], Loss: 1.1009\n",
      "Epoch 74 finished. Loss: 1.1085768415003407\n",
      "Epoch [75/100], Step [1/196], Loss: 0.0105\n",
      "Epoch [75/100], Step [101/196], Loss: 1.0852\n",
      "Epoch 75 finished. Loss: 1.099875239085178\n",
      "Epoch [76/100], Step [1/196], Loss: 0.0099\n",
      "Epoch [76/100], Step [101/196], Loss: 1.0878\n",
      "Epoch 76 finished. Loss: 1.0931939202912\n",
      "Epoch [77/100], Step [1/196], Loss: 0.0113\n",
      "Epoch [77/100], Step [101/196], Loss: 1.0930\n",
      "Epoch 77 finished. Loss: 1.0913960979301103\n",
      "Epoch [78/100], Step [1/196], Loss: 0.0106\n",
      "Epoch [78/100], Step [101/196], Loss: 1.0769\n",
      "Epoch 78 finished. Loss: 1.09053109251723\n",
      "Epoch [79/100], Step [1/196], Loss: 0.0107\n",
      "Epoch [79/100], Step [101/196], Loss: 1.0860\n",
      "Epoch 79 finished. Loss: 1.0873043905107342\n",
      "Epoch [80/100], Step [1/196], Loss: 0.0119\n",
      "Epoch [80/100], Step [101/196], Loss: 1.0819\n",
      "Epoch 80 finished. Loss: 1.0729871690273285\n",
      "Epoch [81/100], Step [1/196], Loss: 0.0112\n",
      "Epoch [81/100], Step [101/196], Loss: 1.0686\n",
      "Epoch 81 finished. Loss: 1.071509309873289\n",
      "Epoch [82/100], Step [1/196], Loss: 0.0099\n",
      "Epoch [82/100], Step [101/196], Loss: 1.0632\n",
      "Epoch 82 finished. Loss: 1.056623939652832\n",
      "Epoch [83/100], Step [1/196], Loss: 0.0109\n",
      "Epoch [83/100], Step [101/196], Loss: 1.0646\n",
      "Epoch 83 finished. Loss: 1.058376092995916\n",
      "Epoch [84/100], Step [1/196], Loss: 0.0103\n",
      "Epoch [84/100], Step [101/196], Loss: 1.0573\n",
      "Epoch 84 finished. Loss: 1.0553827185411842\n",
      "Epoch [85/100], Step [1/196], Loss: 0.0097\n",
      "Epoch [85/100], Step [101/196], Loss: 1.0495\n",
      "Epoch 85 finished. Loss: 1.0473831153037596\n",
      "Epoch [86/100], Step [1/196], Loss: 0.0111\n",
      "Epoch [86/100], Step [101/196], Loss: 1.0432\n",
      "Epoch 86 finished. Loss: 1.0558621859063908\n",
      "Epoch [87/100], Step [1/196], Loss: 0.0103\n",
      "Epoch [87/100], Step [101/196], Loss: 1.0356\n",
      "Epoch 87 finished. Loss: 1.0420034999141887\n",
      "Epoch [88/100], Step [1/196], Loss: 0.0093\n",
      "Epoch [88/100], Step [101/196], Loss: 1.0487\n",
      "Epoch 88 finished. Loss: 1.0417403739934066\n",
      "Epoch [89/100], Step [1/196], Loss: 0.0105\n",
      "Epoch [89/100], Step [101/196], Loss: 1.0326\n",
      "Epoch 89 finished. Loss: 1.0368364328632549\n",
      "Epoch [90/100], Step [1/196], Loss: 0.0101\n",
      "Epoch [90/100], Step [101/196], Loss: 1.0179\n",
      "Epoch 90 finished. Loss: 1.0327934942075185\n",
      "Epoch [91/100], Step [1/196], Loss: 0.0106\n",
      "Epoch [91/100], Step [101/196], Loss: 1.0219\n",
      "Epoch 91 finished. Loss: 1.02740392940385\n",
      "Epoch [92/100], Step [1/196], Loss: 0.0096\n",
      "Epoch [92/100], Step [101/196], Loss: 1.0236\n",
      "Epoch 92 finished. Loss: 1.0187036431565577\n",
      "Epoch [93/100], Step [1/196], Loss: 0.0100\n",
      "Epoch [93/100], Step [101/196], Loss: 1.0183\n",
      "Epoch 93 finished. Loss: 1.0221069220985686\n",
      "Epoch [94/100], Step [1/196], Loss: 0.0108\n",
      "Epoch [94/100], Step [101/196], Loss: 1.0368\n",
      "Epoch 94 finished. Loss: 1.0369869358077342\n",
      "Epoch [95/100], Step [1/196], Loss: 0.0095\n",
      "Epoch [95/100], Step [101/196], Loss: 1.0154\n",
      "Epoch 95 finished. Loss: 1.0146946621184447\n",
      "Epoch [96/100], Step [1/196], Loss: 0.0086\n",
      "Epoch [96/100], Step [101/196], Loss: 1.0366\n",
      "Epoch 96 finished. Loss: 1.0432908188323586\n",
      "Epoch [97/100], Step [1/196], Loss: 0.0097\n",
      "Epoch [97/100], Step [101/196], Loss: 1.0443\n",
      "Epoch 97 finished. Loss: 1.04009404291912\n",
      "Epoch [98/100], Step [1/196], Loss: 0.0098\n",
      "Epoch [98/100], Step [101/196], Loss: 1.0015\n",
      "Epoch 98 finished. Loss: 1.0059690743076557\n",
      "Epoch [99/100], Step [1/196], Loss: 0.0113\n",
      "Epoch [99/100], Step [101/196], Loss: 1.0081\n",
      "Epoch 99 finished. Loss: 1.0168338056121553\n",
      "Epoch [100/100], Step [1/196], Loss: 0.0109\n",
      "Epoch [100/100], Step [101/196], Loss: 1.0460\n",
      "Epoch 100 finished. Loss: 1.039133398508539\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(),lr = 0.003)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    for i, (inputs,labels) in enumerate(training_loader):\n",
    "        inputs,labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # print(f'batch {i} completed')\n",
    "\n",
    "        running_loss += loss.item() # For monitoring\n",
    "        epoch_loss += loss.item() # For plot\n",
    "\n",
    "        if(i % __n == 0):\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(training_loader)}], Loss: {running_loss / __n:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "    epoch_loss = epoch_loss / len(training_loader)\n",
    "    print(f\"Epoch {epoch+1} finished. Loss: {epoch_loss}\")\n",
    "    _training_losses.append(epoch_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732008570475,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "DZtxu_vtTSuI"
   },
   "outputs": [],
   "source": [
    "_end = time.time()\n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_PATH, f'final_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732008570475,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "GNGyK4LmfrPI",
    "outputId": "69e034bd-98a1-4faa-b086-e5d77f87c2fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1732008570.305466\n"
     ]
    }
   ],
   "source": [
    "print(_end)\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_PATH, 'trainning_time.txt'), 'w') as f:\n",
    "  f.write(f\"Start Time: {_start}\\nEnd Time:{_end}\\nTotal Time:{_end - _start}\")\n",
    "\n",
    "\n",
    "DF = pd.DataFrame(np.array(_training_losses))\n",
    "DF.to_csv(os.path.join(CHECKPOINT_PATH, 'losses.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 95,
     "status": "ok",
     "timestamp": 1732008570568,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "eWOCOV2428mZ",
    "outputId": "961f5721-683c-4b88-bce7-dc5bc9be4422"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "===============================================================================================\n",
       "Layer (type:depth-idx)                        Output Shape              Param #\n",
       "===============================================================================================\n",
       "Vim                                           [256, 10]                 96\n",
       "├─Sequential: 1-1                             [256, 64, 96]             --\n",
       "│    └─Rearrange: 2-1                         [256, 64, 48]             --\n",
       "│    └─Linear: 2-2                            [256, 64, 96]             4,704\n",
       "├─Dropout: 1-2                                [256, 64, 96]             --\n",
       "├─ModuleList: 1-3                             --                        --\n",
       "│    └─VisionEncoderMambaBlock: 2-3           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-1                    [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-2                       [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-3                         [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-4                       [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-5                       [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-6                     [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-7                          [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-8                       [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-9                     [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-10                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-11                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-4           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-12                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-13                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-14                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-15                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-16                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-17                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-18                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-19                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-20                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-21                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-22                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-5           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-23                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-24                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-25                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-26                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-27                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-28                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-29                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-30                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-31                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-32                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-33                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-6           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-34                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-35                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-36                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-37                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-38                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-39                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-40                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-41                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-42                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-43                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-44                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-7           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-45                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-46                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-47                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-48                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-49                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-50                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-51                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-52                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-53                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-54                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-55                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-8           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-56                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-57                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-58                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-59                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-60                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-61                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-62                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-63                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-64                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-65                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-66                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-9           [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-67                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-68                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-69                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-70                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-71                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-72                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-73                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-74                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-75                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-76                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-77                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-10          [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-78                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-79                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-80                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-81                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-82                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-83                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-84                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-85                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-86                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-87                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-88                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-11          [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-89                   [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-90                      [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-91                        [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-92                      [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-93                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-94                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-95                         [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-96                      [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-97                    [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-98                         [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-99                     [256, 64, 96]             4,608\n",
       "│    └─VisionEncoderMambaBlock: 2-12          [256, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-100                  [256, 64, 96]             192\n",
       "│    │    └─Linear: 3-101                     [256, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-102                       [256, 64, 96]             --\n",
       "│    │    └─Linear: 3-103                     [256, 64, 96]             (recursive)\n",
       "│    │    └─Conv1d: 3-104                     [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-105                   [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-106                        [256, 64, 96]             30,912\n",
       "│    │    └─Conv1d: 3-107                     [256, 96, 64]             9,312\n",
       "│    │    └─Softplus: 3-108                   [256, 96, 64]             --\n",
       "│    │    └─SSM: 3-109                        [256, 64, 96]             (recursive)\n",
       "│    │    └─SEBlock: 3-110                    [256, 64, 96]             4,608\n",
       "├─Identity: 1-4                               [256, 64, 96]             --\n",
       "├─Sequential: 1-5                             [256, 10]                 --\n",
       "│    └─Reduce: 2-13                           [256, 96]                 --\n",
       "│    └─LayerNorm: 2-14                        [256, 96]                 192\n",
       "│    └─Linear: 2-15                           [256, 10]                 970\n",
       "===============================================================================================\n",
       "Total params: 642,442\n",
       "Trainable params: 642,442\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 3.22\n",
       "===============================================================================================\n",
       "Input size (MB): 3.15\n",
       "Forward/backward pass size (MB): 1441.32\n",
       "Params size (MB): 2.20\n",
       "Estimated Total Size (MB): 1446.66\n",
       "==============================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(batch_size, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10979,
     "status": "ok",
     "timestamp": 1732008581546,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "Qhg5IBirMhdO",
    "outputId": "44e69e9d-1c2f-4ed3-c4af-353ce465345a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model test images: 61.01%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs,labels in testing_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"accuracy of the model test images: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 0,
     "status": "ok",
     "timestamp": 1732008581554,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "jeE1IeuDNjro",
    "outputId": "c6a8677a-d5a1-4395-adf3-435575d500a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  29998 KiB |  38818 MiB |   5444 TiB |   5444 TiB |\n",
      "|       from large pool |  19712 KiB |  38806 MiB |   5443 TiB |   5443 TiB |\n",
      "|       from small pool |  10286 KiB |     14 MiB |      1 TiB |      1 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  29998 KiB |  38818 MiB |   5444 TiB |   5444 TiB |\n",
      "|       from large pool |  19712 KiB |  38806 MiB |   5443 TiB |   5443 TiB |\n",
      "|       from small pool |  10286 KiB |     14 MiB |      1 TiB |      1 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  29956 KiB |  38815 MiB |   5443 TiB |   5443 TiB |\n",
      "|       from large pool |  19712 KiB |  38803 MiB |   5442 TiB |   5442 TiB |\n",
      "|       from small pool |  10244 KiB |     14 MiB |      1 TiB |      1 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  39290 MiB |  39290 MiB |  39290 MiB |      0 B   |\n",
      "|       from large pool |  39274 MiB |  39274 MiB |  39274 MiB |      0 B   |\n",
      "|       from small pool |     16 MiB |     16 MiB |     16 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 299729 KiB |   2193 MiB | 655673 GiB | 655673 GiB |\n",
      "|       from large pool | 295680 KiB |   2190 MiB | 654451 GiB | 654451 GiB |\n",
      "|       from small pool |   4049 KiB |      6 MiB |   1222 GiB |   1222 GiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     638    |     843    |   48011 K  |   48011 K  |\n",
      "|       from large pool |       3    |     272    |   33124 K  |   33124 K  |\n",
      "|       from small pool |     635    |     788    |   14887 K  |   14886 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     638    |     843    |   48011 K  |   48011 K  |\n",
      "|       from large pool |       3    |     272    |   33124 K  |   33124 K  |\n",
      "|       from small pool |     635    |     788    |   14887 K  |   14886 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      85    |      85    |      85    |       0    |\n",
      "|       from large pool |      77    |      77    |      77    |       0    |\n",
      "|       from small pool |       8    |       8    |       8    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      28    |      64    |   16397 K  |   16397 K  |\n",
      "|       from large pool |       3    |      32    |   10935 K  |   10935 K  |\n",
      "|       from small pool |      25    |      45    |    5461 K  |    5461 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
