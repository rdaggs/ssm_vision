{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 937,
     "status": "ok",
     "timestamp": 1731996927159,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "Dh-HVly3UbmJ",
    "outputId": "f67a27e3-0a61-4daa-8c93-c136e1c7d1f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "# Google Drive + working directory path\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "PATH = '/content/drive/MyDrive/Vision24/'\n",
    "VERSION = 'MobileNetV2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731996927159,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "E06Oj02_U9YW",
    "outputId": "1583ea11-c4f7-4394-eba1-f56b16da047d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.path.isdir(PATH))\n",
    "assert os.path.isdir(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1731996927159,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "VcUeV6IWV_XH",
    "outputId": "d6f8aaed-da11-4c3d-ef45-f76f6f27bc3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "CHECKPOINT_PATH = os.path.join(PATH, VERSION)\n",
    "os.makedirs(CHECKPOINT_PATH, exist_ok=True)\n",
    "\n",
    "\n",
    "print(os.path.isdir(CHECKPOINT_PATH))\n",
    "assert os.path.isdir(CHECKPOINT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 206,
     "status": "ok",
     "timestamp": 1731996927362,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "ZsVjFoYbpMLY",
    "outputId": "e444fada-14dd-448d-def9-5fee722879f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Nov 19 06:15:27 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100-SXM4-40GB          Off | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   43C    P0              47W / 400W |      2MiB / 40960MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "# verify >16GB of RAM\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7994,
     "status": "ok",
     "timestamp": 1731996935355,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "-H3kp0n20z4P"
   },
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install zetascale\n",
    "!pip install swarms\n",
    "!pip install torchinfo\n",
    "\n",
    "import torch\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from torch import nn, Tensor\n",
    "from zeta.nn import SSM\n",
    "from einops.layers.torch import Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731996935356,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "TdMgDLMzkq0x"
   },
   "outputs": [],
   "source": [
    "# Squeeze and Excitation block\n",
    "from torch.nn import AdaptiveAvgPool1d, Linear, Sigmoid, ReLU\n",
    "\n",
    "class MobileNetV2_DepthwiseConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, expansion_factor=4):\n",
    "        super(MobileNetV2_DepthwiseConv, self).__init__()\n",
    "\n",
    "        # Calculate the number of expanded channels (for the bottleneck structure)\n",
    "        self.expansion_channels = in_channels * expansion_factor\n",
    "\n",
    "        # Step 1: Expansion Layer\n",
    "        self.expand_conv = nn.Conv1d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=self.expansion_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.expand_bn = nn.BatchNorm1d(self.expansion_channels)  # Normalize the expanded channels\n",
    "        self.expand_activation = nn.SiLU()  # Non-linear activation after expansion\n",
    "\n",
    "\n",
    "        # Step 2: Depthwise Convolution\n",
    "        self.depthwise_conv = nn.Conv1d(\n",
    "            in_channels=self.expansion_channels,\n",
    "            out_channels=self.expansion_channels,\n",
    "            kernel_size=3,\n",
    "            stride=1,\n",
    "            padding=1,\n",
    "            groups=self.expansion_channels,  # Depthwise convolution\n",
    "            bias=False\n",
    "        )\n",
    "        self.depthwise_bn = nn.BatchNorm1d(self.expansion_channels)  # Normalize after depthwise convolution\n",
    "        self.depthwise_activation = nn.SiLU()  # Non-linear activation\n",
    "\n",
    "\n",
    "        # Step 3: Projection Layer\n",
    "        self.project_conv = nn.Conv1d(\n",
    "            in_channels=self.expansion_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            bias=False\n",
    "        )\n",
    "        self.project_bn = nn.BatchNorm1d(out_channels)  # Normalize the projected output\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        skip = x  # Save input for residual connection (if applicable)\n",
    "\n",
    "        # Step 1: Expand input channels\n",
    "        x = self.expand_conv(x)\n",
    "        x = self.expand_bn(x)\n",
    "        x = self.expand_activation(x)\n",
    "\n",
    "        # Step 2: Apply depthwise convolution\n",
    "        x = self.depthwise_conv(x)\n",
    "        x = self.depthwise_bn(x)\n",
    "        x = self.depthwise_activation(x)\n",
    "\n",
    "        # Step 3: Project back to the original dimensionality\n",
    "        x = self.project_conv(x)\n",
    "        x = self.project_bn(x)\n",
    "\n",
    "        # Add residual connection\n",
    "        if skip.shape == x.shape:\n",
    "            x += skip\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1731996935356,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "b-qEXTAD0rU8"
   },
   "outputs": [],
   "source": [
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "def output_head(dim: int, num_classes: int):\n",
    "    \"\"\"\n",
    "    Creates a head for the output layer of a model.\n",
    "\n",
    "    Args:\n",
    "        dim (int): The input dimension of the head.\n",
    "        num_classes (int): The number of output classes.\n",
    "\n",
    "    Returns:\n",
    "        nn.Sequential: The output head module.\n",
    "    \"\"\"\n",
    "    return nn.Sequential(\n",
    "        Reduce(\"b s d -> b d\", \"mean\"),\n",
    "        nn.LayerNorm(dim),\n",
    "        nn.Linear(dim, num_classes),\n",
    "    )\n",
    "\n",
    "\n",
    "class VisionEncoderMambaBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dt_rank: int,\n",
    "        dim_inner: int,\n",
    "        d_state: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "\n",
    "        # Depthwise separable convolutions\n",
    "        self.forward_conv = MobileNetV2_DepthwiseConv(in_channels=dim, out_channels=dim)\n",
    "        self.backward_conv = MobileNetV2_DepthwiseConv(in_channels=dim, out_channels=dim)\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.silu = nn.SiLU()\n",
    "        self.ssm = SSM(dim, dt_rank, dim_inner, d_state)\n",
    "\n",
    "        self.proj = nn.Linear(dim, dim) # projection layer\n",
    "        self.softplus = nn.Softplus()\n",
    "\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        b, s, d = x.shape\n",
    "\n",
    "        skip = x            # skip connection\n",
    "        x = self.norm(x)\n",
    "        z = self.silu(self.proj(x))   # project --> activation for gating\n",
    "        x = self.proj(x)\n",
    "\n",
    "        x1 = self.process_direction(x, self.forward_conv, self.ssm,)\n",
    "        x2 = self.process_direction(x, self.backward_conv, self.ssm,)\n",
    "\n",
    "        x1 *= z\n",
    "        x2 *= z\n",
    "\n",
    "        x = x1 + x2\n",
    "        return x + skip\n",
    "\n",
    "    def process_direction(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        conv1d: nn.Conv1d,\n",
    "        ssm: SSM,\n",
    "    ):\n",
    "        x = rearrange(x, \"b s d -> b d s\")\n",
    "        x = self.softplus(conv1d(x))\n",
    "        x = rearrange(x, \"b d s -> b s d\")\n",
    "        x = ssm(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Vim(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        dt_rank: int = 32,\n",
    "        dim_inner: int = None,\n",
    "        d_state: int = None,\n",
    "        num_classes: int = None,\n",
    "        image_size: int = 224,\n",
    "        patch_size: int = 16,\n",
    "        channels: int = 3,\n",
    "        dropout: float = 0.1,\n",
    "        depth: int = 12,\n",
    "        *args,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.dt_rank = dt_rank\n",
    "        self.dim_inner = dim_inner\n",
    "        self.d_state = d_state\n",
    "        self.num_classes = num_classes\n",
    "        self.image_size = image_size\n",
    "        self.patch_size = patch_size\n",
    "        self.channels = channels\n",
    "        self.dropout = dropout\n",
    "        self.depth = depth\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        patch_dim = channels * patch_height * patch_width\n",
    "\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            Rearrange(\n",
    "                \"b c (h p1) (w p2) -> b (h w) (p1 p2 c)\",\n",
    "                p1=patch_height,\n",
    "                p2=patch_height,\n",
    "            ),\n",
    "            nn.Linear(patch_dim, dim),\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.to_latent = nn.Identity()\n",
    "        self.layers = nn.ModuleList()        # encoder layers\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                VisionEncoderMambaBlock(\n",
    "                    dim=dim,\n",
    "                    dt_rank=dt_rank,\n",
    "                    dim_inner=dim_inner,\n",
    "                    d_state=d_state,\n",
    "                    *args,\n",
    "                    **kwargs,\n",
    "                )\n",
    "            )\n",
    "        self.output_head = output_head(dim, num_classes)\n",
    "\n",
    "    def forward(self, x: Tensor):\n",
    "        b, c, h, w = x.shape\n",
    "        x = self.to_patch_embedding(x)\n",
    "        b, n, _ = x.shape\n",
    "        cls_tokens = repeat(self.cls_token, \"() n d -> b n d\", b=b)\n",
    "        # x = torch.cat((cls_tokens, x), dim=1)\n",
    "        x = self.dropout(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "        x = self.to_latent(x)\n",
    "        # x = reduce(x, \"b s d -> b d\", \"mean\")\n",
    "        return self.output_head(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1766,
     "status": "ok",
     "timestamp": 1731996937118,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "OYfhf42jJcxS",
    "outputId": "a5335b91-e80b-44a4-cfde-303c5031ebd2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from einops import repeat\n",
    "\n",
    "model = Vim(dim=96,\n",
    "            dt_rank=16,\n",
    "            dim_inner=96,\n",
    "            d_state=96,\n",
    "            num_classes=10,\n",
    "            image_size=32,\n",
    "            patch_size=4,\n",
    "            channels=3,\n",
    "            dropout=0.1,\n",
    "            depth=10,)\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize([224,224]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))\n",
    "])\n",
    "\n",
    "training_data = torchvision.datasets.CIFAR10(root = './dataa',train=True,download=True,transform=transform)\n",
    "testing_data = torchvision.datasets.CIFAR10(root = './data',train=False,download=True,transform=transform)\n",
    "\n",
    "training_loader = DataLoader(training_data, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "testing_loader = DataLoader(testing_data, batch_size=batch_size, shuffle=True, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1731996937118,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "IfWldubxTfy_",
    "outputId": "5a51db83-a792-42d7-c458-5864888c3e23"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs:  100\n",
      "Training data:  391\n",
      "Total steps:  39100\n",
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print('epochs: ', epochs)\n",
    "print('Training data: ', len(training_loader))\n",
    "print('Total steps: ', epochs*len(training_loader))\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1731996937118,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "tO2AQTt3gcE7",
    "outputId": "c3a83c5b-b473-41c6-8a02-ab9329bce9c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1731996936.9581318\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "_start = time.time()\n",
    "print(_start)\n",
    "\n",
    "_training_losses = []\n",
    "__n = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7794644,
     "status": "ok",
     "timestamp": 1732013514206,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "x6wPqp21LIyF",
    "outputId": "02efd85f-cdb6-46f9-d060-f9e9a0f7cb27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Step [100/391], Loss: 1.9594\n",
      "Epoch [1/100], Step [200/391], Loss: 1.7235\n",
      "Epoch [1/100], Step [300/391], Loss: 1.6260\n",
      "Epoch 1 finished. Loss: 1.7170702795238446\n",
      "Epoch [2/100], Step [100/391], Loss: 1.4823\n",
      "Epoch [2/100], Step [200/391], Loss: 1.3970\n",
      "Epoch [2/100], Step [300/391], Loss: 1.3747\n",
      "Epoch 2 finished. Loss: 1.3978873533970864\n",
      "Epoch [3/100], Step [100/391], Loss: 1.2691\n",
      "Epoch [3/100], Step [200/391], Loss: 1.2665\n",
      "Epoch [3/100], Step [300/391], Loss: 1.2338\n",
      "Epoch 3 finished. Loss: 1.247247184176579\n",
      "Epoch [4/100], Step [100/391], Loss: 1.1692\n",
      "Epoch [4/100], Step [200/391], Loss: 1.1555\n",
      "Epoch [4/100], Step [300/391], Loss: 1.1428\n",
      "Epoch 4 finished. Loss: 1.1573464629595236\n",
      "Epoch [5/100], Step [100/391], Loss: 1.0822\n",
      "Epoch [5/100], Step [200/391], Loss: 1.0968\n",
      "Epoch [5/100], Step [300/391], Loss: 1.0845\n",
      "Epoch 5 finished. Loss: 1.0835438567354245\n",
      "Epoch [6/100], Step [100/391], Loss: 1.0093\n",
      "Epoch [6/100], Step [200/391], Loss: 1.0142\n",
      "Epoch [6/100], Step [300/391], Loss: 1.0412\n",
      "Epoch 6 finished. Loss: 1.0255953649730634\n",
      "Epoch [7/100], Step [100/391], Loss: 0.9474\n",
      "Epoch [7/100], Step [200/391], Loss: 0.9584\n",
      "Epoch [7/100], Step [300/391], Loss: 0.9739\n",
      "Epoch 7 finished. Loss: 0.9614335217744189\n",
      "Epoch [8/100], Step [100/391], Loss: 0.8788\n",
      "Epoch [8/100], Step [200/391], Loss: 0.8980\n",
      "Epoch [8/100], Step [300/391], Loss: 0.9128\n",
      "Epoch 8 finished. Loss: 0.9028880286704549\n",
      "Epoch [9/100], Step [100/391], Loss: 0.8197\n",
      "Epoch [9/100], Step [200/391], Loss: 0.8309\n",
      "Epoch [9/100], Step [300/391], Loss: 0.8798\n",
      "Epoch 9 finished. Loss: 0.8461984345675124\n",
      "Epoch [10/100], Step [100/391], Loss: 0.7493\n",
      "Epoch [10/100], Step [200/391], Loss: 0.7922\n",
      "Epoch [10/100], Step [300/391], Loss: 0.7973\n",
      "Epoch 10 finished. Loss: 0.789953122968259\n",
      "Epoch [11/100], Step [100/391], Loss: 0.7304\n",
      "Epoch [11/100], Step [200/391], Loss: 0.7409\n",
      "Epoch [11/100], Step [300/391], Loss: 0.7566\n",
      "Epoch 11 finished. Loss: 0.749933800276588\n",
      "Epoch [12/100], Step [100/391], Loss: 0.6453\n",
      "Epoch [12/100], Step [200/391], Loss: 0.6879\n",
      "Epoch [12/100], Step [300/391], Loss: 0.7220\n",
      "Epoch 12 finished. Loss: 0.6983252667709995\n",
      "Epoch [13/100], Step [100/391], Loss: 0.5975\n",
      "Epoch [13/100], Step [200/391], Loss: 0.6388\n",
      "Epoch [13/100], Step [300/391], Loss: 0.6629\n",
      "Epoch 13 finished. Loss: 0.6440177335763526\n",
      "Epoch [14/100], Step [100/391], Loss: 0.5388\n",
      "Epoch [14/100], Step [200/391], Loss: 0.5982\n",
      "Epoch [14/100], Step [300/391], Loss: 0.6098\n",
      "Epoch 14 finished. Loss: 0.5922257875085182\n",
      "Epoch [15/100], Step [100/391], Loss: 0.5077\n",
      "Epoch [15/100], Step [200/391], Loss: 0.5480\n",
      "Epoch [15/100], Step [300/391], Loss: 0.5641\n",
      "Epoch 15 finished. Loss: 0.5473893566052322\n",
      "Epoch [16/100], Step [100/391], Loss: 0.4538\n",
      "Epoch [16/100], Step [200/391], Loss: 0.5087\n",
      "Epoch [16/100], Step [300/391], Loss: 0.5495\n",
      "Epoch 16 finished. Loss: 0.5167837556060928\n",
      "Epoch [17/100], Step [100/391], Loss: 0.4056\n",
      "Epoch [17/100], Step [200/391], Loss: 0.4422\n",
      "Epoch [17/100], Step [300/391], Loss: 0.4902\n",
      "Epoch 17 finished. Loss: 0.4585583442464814\n",
      "Epoch [18/100], Step [100/391], Loss: 0.3720\n",
      "Epoch [18/100], Step [200/391], Loss: 0.4182\n",
      "Epoch [18/100], Step [300/391], Loss: 0.4549\n",
      "Epoch 18 finished. Loss: 0.4297180469612331\n",
      "Epoch [19/100], Step [100/391], Loss: 0.3761\n",
      "Epoch [19/100], Step [200/391], Loss: 0.3968\n",
      "Epoch [19/100], Step [300/391], Loss: 0.4101\n",
      "Epoch 19 finished. Loss: 0.39950441147970117\n",
      "Epoch [20/100], Step [100/391], Loss: 0.3076\n",
      "Epoch [20/100], Step [200/391], Loss: 0.3325\n",
      "Epoch [20/100], Step [300/391], Loss: 0.3577\n",
      "Epoch 20 finished. Loss: 0.35035143301005256\n",
      "Epoch [21/100], Step [100/391], Loss: 0.2939\n",
      "Epoch [21/100], Step [200/391], Loss: 0.3070\n",
      "Epoch [21/100], Step [300/391], Loss: 0.3528\n",
      "Epoch 21 finished. Loss: 0.3220020994124815\n",
      "Epoch [22/100], Step [100/391], Loss: 0.2450\n",
      "Epoch [22/100], Step [200/391], Loss: 0.2784\n",
      "Epoch [22/100], Step [300/391], Loss: 0.3222\n",
      "Epoch 22 finished. Loss: 0.29459898333872675\n",
      "Epoch [23/100], Step [100/391], Loss: 0.2311\n",
      "Epoch [23/100], Step [200/391], Loss: 0.2575\n",
      "Epoch [23/100], Step [300/391], Loss: 0.3007\n",
      "Epoch 23 finished. Loss: 0.2777191475224312\n",
      "Epoch [24/100], Step [100/391], Loss: 0.2181\n",
      "Epoch [24/100], Step [200/391], Loss: 0.2175\n",
      "Epoch [24/100], Step [300/391], Loss: 0.2458\n",
      "Epoch 24 finished. Loss: 0.2417322740034984\n",
      "Epoch [25/100], Step [100/391], Loss: 0.1899\n",
      "Epoch [25/100], Step [200/391], Loss: 0.2111\n",
      "Epoch [25/100], Step [300/391], Loss: 0.2326\n",
      "Epoch 25 finished. Loss: 0.22508064524063368\n",
      "Epoch [26/100], Step [100/391], Loss: 0.1980\n",
      "Epoch [26/100], Step [200/391], Loss: 0.1834\n",
      "Epoch [26/100], Step [300/391], Loss: 0.2179\n",
      "Epoch 26 finished. Loss: 0.20931702084324855\n",
      "Epoch [27/100], Step [100/391], Loss: 0.1827\n",
      "Epoch [27/100], Step [200/391], Loss: 0.1808\n",
      "Epoch [27/100], Step [300/391], Loss: 0.2175\n",
      "Epoch 27 finished. Loss: 0.2001185408600456\n",
      "Epoch [28/100], Step [100/391], Loss: 0.1674\n",
      "Epoch [28/100], Step [200/391], Loss: 0.1657\n",
      "Epoch [28/100], Step [300/391], Loss: 0.1900\n",
      "Epoch 28 finished. Loss: 0.18426850624858876\n",
      "Epoch [29/100], Step [100/391], Loss: 0.1464\n",
      "Epoch [29/100], Step [200/391], Loss: 0.1466\n",
      "Epoch [29/100], Step [300/391], Loss: 0.1717\n",
      "Epoch 29 finished. Loss: 0.16651579272716552\n",
      "Epoch [30/100], Step [100/391], Loss: 0.1429\n",
      "Epoch [30/100], Step [200/391], Loss: 0.1643\n",
      "Epoch [30/100], Step [300/391], Loss: 0.1610\n",
      "Epoch 30 finished. Loss: 0.16173902116811184\n",
      "Epoch [31/100], Step [100/391], Loss: 0.1642\n",
      "Epoch [31/100], Step [200/391], Loss: 0.1643\n",
      "Epoch [31/100], Step [300/391], Loss: 0.1826\n",
      "Epoch 31 finished. Loss: 0.17037772488258685\n",
      "Epoch [32/100], Step [100/391], Loss: 0.1220\n",
      "Epoch [32/100], Step [200/391], Loss: 0.1226\n",
      "Epoch [32/100], Step [300/391], Loss: 0.1219\n",
      "Epoch 32 finished. Loss: 0.13170872997406805\n",
      "Epoch [33/100], Step [100/391], Loss: 0.1261\n",
      "Epoch [33/100], Step [200/391], Loss: 0.1191\n",
      "Epoch [33/100], Step [300/391], Loss: 0.1407\n",
      "Epoch 33 finished. Loss: 0.1340354262848797\n",
      "Epoch [34/100], Step [100/391], Loss: 0.1220\n",
      "Epoch [34/100], Step [200/391], Loss: 0.1334\n",
      "Epoch [34/100], Step [300/391], Loss: 0.1260\n",
      "Epoch 34 finished. Loss: 0.13136992588296265\n",
      "Epoch [35/100], Step [100/391], Loss: 0.1114\n",
      "Epoch [35/100], Step [200/391], Loss: 0.1157\n",
      "Epoch [35/100], Step [300/391], Loss: 0.1367\n",
      "Epoch 35 finished. Loss: 0.12902296839467706\n",
      "Epoch [36/100], Step [100/391], Loss: 0.1174\n",
      "Epoch [36/100], Step [200/391], Loss: 0.1109\n",
      "Epoch [36/100], Step [300/391], Loss: 0.1233\n",
      "Epoch 36 finished. Loss: 0.12614288664115664\n",
      "Epoch [37/100], Step [100/391], Loss: 0.1053\n",
      "Epoch [37/100], Step [200/391], Loss: 0.1045\n",
      "Epoch [37/100], Step [300/391], Loss: 0.1183\n",
      "Epoch 37 finished. Loss: 0.11263260607371854\n",
      "Epoch [38/100], Step [100/391], Loss: 0.1046\n",
      "Epoch [38/100], Step [200/391], Loss: 0.1144\n",
      "Epoch [38/100], Step [300/391], Loss: 0.1088\n",
      "Epoch 38 finished. Loss: 0.11743331357331761\n",
      "Epoch [39/100], Step [100/391], Loss: 0.0905\n",
      "Epoch [39/100], Step [200/391], Loss: 0.1071\n",
      "Epoch [39/100], Step [300/391], Loss: 0.1185\n",
      "Epoch 39 finished. Loss: 0.10872308459714093\n",
      "Epoch [40/100], Step [100/391], Loss: 0.0975\n",
      "Epoch [40/100], Step [200/391], Loss: 0.1196\n",
      "Epoch [40/100], Step [300/391], Loss: 0.1393\n",
      "Epoch 40 finished. Loss: 0.12484674305771776\n",
      "Epoch [41/100], Step [100/391], Loss: 0.1073\n",
      "Epoch [41/100], Step [200/391], Loss: 0.0886\n",
      "Epoch [41/100], Step [300/391], Loss: 0.0925\n",
      "Epoch 41 finished. Loss: 0.103324295224055\n",
      "Epoch [42/100], Step [100/391], Loss: 0.0952\n",
      "Epoch [42/100], Step [200/391], Loss: 0.0872\n",
      "Epoch [42/100], Step [300/391], Loss: 0.0951\n",
      "Epoch 42 finished. Loss: 0.0968049423068838\n",
      "Epoch [43/100], Step [100/391], Loss: 0.0887\n",
      "Epoch [43/100], Step [200/391], Loss: 0.0939\n",
      "Epoch [43/100], Step [300/391], Loss: 0.1039\n",
      "Epoch 43 finished. Loss: 0.09821395426416946\n",
      "Epoch [44/100], Step [100/391], Loss: 0.0880\n",
      "Epoch [44/100], Step [200/391], Loss: 0.0869\n",
      "Epoch [44/100], Step [300/391], Loss: 0.1069\n",
      "Epoch 44 finished. Loss: 0.10477246017769322\n",
      "Epoch [45/100], Step [100/391], Loss: 0.0967\n",
      "Epoch [45/100], Step [200/391], Loss: 0.0990\n",
      "Epoch [45/100], Step [300/391], Loss: 0.0951\n",
      "Epoch 45 finished. Loss: 0.09851342365812615\n",
      "Epoch [46/100], Step [100/391], Loss: 0.0761\n",
      "Epoch [46/100], Step [200/391], Loss: 0.0863\n",
      "Epoch [46/100], Step [300/391], Loss: 0.0929\n",
      "Epoch 46 finished. Loss: 0.0880439991341032\n",
      "Epoch [47/100], Step [100/391], Loss: 0.0884\n",
      "Epoch [47/100], Step [200/391], Loss: 0.0891\n",
      "Epoch [47/100], Step [300/391], Loss: 0.0987\n",
      "Epoch 47 finished. Loss: 0.09489283055695884\n",
      "Epoch [48/100], Step [100/391], Loss: 0.0878\n",
      "Epoch [48/100], Step [200/391], Loss: 0.0988\n",
      "Epoch [48/100], Step [300/391], Loss: 0.0975\n",
      "Epoch 48 finished. Loss: 0.09510582795037943\n",
      "Epoch [49/100], Step [100/391], Loss: 0.0858\n",
      "Epoch [49/100], Step [200/391], Loss: 0.0924\n",
      "Epoch [49/100], Step [300/391], Loss: 0.1058\n",
      "Epoch 49 finished. Loss: 0.09629193440441737\n",
      "Epoch [50/100], Step [100/391], Loss: 0.0824\n",
      "Epoch [50/100], Step [200/391], Loss: 0.0770\n",
      "Epoch [50/100], Step [300/391], Loss: 0.0694\n",
      "Epoch 50 finished. Loss: 0.07944806456051366\n",
      "Epoch [51/100], Step [100/391], Loss: 0.0841\n",
      "Epoch [51/100], Step [200/391], Loss: 0.0789\n",
      "Epoch [51/100], Step [300/391], Loss: 0.0861\n",
      "Epoch 51 finished. Loss: 0.08848848842475039\n",
      "Epoch [52/100], Step [100/391], Loss: 0.0894\n",
      "Epoch [52/100], Step [200/391], Loss: 0.0905\n",
      "Epoch [52/100], Step [300/391], Loss: 0.0855\n",
      "Epoch 52 finished. Loss: 0.09105947729476425\n",
      "Epoch [53/100], Step [100/391], Loss: 0.0823\n",
      "Epoch [53/100], Step [200/391], Loss: 0.0774\n",
      "Epoch [53/100], Step [300/391], Loss: 0.0750\n",
      "Epoch 53 finished. Loss: 0.07576075133622226\n",
      "Epoch [54/100], Step [100/391], Loss: 0.0560\n",
      "Epoch [54/100], Step [200/391], Loss: 0.0604\n",
      "Epoch [54/100], Step [300/391], Loss: 0.0794\n",
      "Epoch 54 finished. Loss: 0.06977727741022091\n",
      "Epoch [55/100], Step [100/391], Loss: 0.0639\n",
      "Epoch [55/100], Step [200/391], Loss: 0.0748\n",
      "Epoch [55/100], Step [300/391], Loss: 0.0855\n",
      "Epoch 55 finished. Loss: 0.07742927055281904\n",
      "Epoch [56/100], Step [100/391], Loss: 0.0847\n",
      "Epoch [56/100], Step [200/391], Loss: 0.0801\n",
      "Epoch [56/100], Step [300/391], Loss: 0.0783\n",
      "Epoch 56 finished. Loss: 0.08272049447183338\n",
      "Epoch [57/100], Step [100/391], Loss: 0.0900\n",
      "Epoch [57/100], Step [200/391], Loss: 0.0897\n",
      "Epoch [57/100], Step [300/391], Loss: 0.0933\n",
      "Epoch 57 finished. Loss: 0.09217652297147629\n",
      "Epoch [58/100], Step [100/391], Loss: 0.0686\n",
      "Epoch [58/100], Step [200/391], Loss: 0.0586\n",
      "Epoch [58/100], Step [300/391], Loss: 0.0710\n",
      "Epoch 58 finished. Loss: 0.07152628718787218\n",
      "Epoch [59/100], Step [100/391], Loss: 0.0574\n",
      "Epoch [59/100], Step [200/391], Loss: 0.0736\n",
      "Epoch [59/100], Step [300/391], Loss: 0.0780\n",
      "Epoch 59 finished. Loss: 0.07296945589363499\n",
      "Epoch [60/100], Step [100/391], Loss: 0.0743\n",
      "Epoch [60/100], Step [200/391], Loss: 0.0685\n",
      "Epoch [60/100], Step [300/391], Loss: 0.0757\n",
      "Epoch 60 finished. Loss: 0.07383697677422743\n",
      "Epoch [61/100], Step [100/391], Loss: 0.0575\n",
      "Epoch [61/100], Step [200/391], Loss: 0.0688\n",
      "Epoch [61/100], Step [300/391], Loss: 0.0705\n",
      "Epoch 61 finished. Loss: 0.0688031298106017\n",
      "Epoch [62/100], Step [100/391], Loss: 0.0685\n",
      "Epoch [62/100], Step [200/391], Loss: 0.0659\n",
      "Epoch [62/100], Step [300/391], Loss: 0.0602\n",
      "Epoch 62 finished. Loss: 0.06309097036814598\n",
      "Epoch [63/100], Step [100/391], Loss: 0.0538\n",
      "Epoch [63/100], Step [200/391], Loss: 0.0603\n",
      "Epoch [63/100], Step [300/391], Loss: 0.0648\n",
      "Epoch 63 finished. Loss: 0.0655455594034413\n",
      "Epoch [64/100], Step [100/391], Loss: 0.0641\n",
      "Epoch [64/100], Step [200/391], Loss: 0.0666\n",
      "Epoch [64/100], Step [300/391], Loss: 0.0886\n",
      "Epoch 64 finished. Loss: 0.07734849606461995\n",
      "Epoch [65/100], Step [100/391], Loss: 0.0845\n",
      "Epoch [65/100], Step [200/391], Loss: 0.0755\n",
      "Epoch [65/100], Step [300/391], Loss: 0.0655\n",
      "Epoch 65 finished. Loss: 0.07435087191507868\n",
      "Epoch [66/100], Step [100/391], Loss: 0.0537\n",
      "Epoch [66/100], Step [200/391], Loss: 0.0610\n",
      "Epoch [66/100], Step [300/391], Loss: 0.0636\n",
      "Epoch 66 finished. Loss: 0.06374450551960474\n",
      "Epoch [67/100], Step [100/391], Loss: 0.0770\n",
      "Epoch [67/100], Step [200/391], Loss: 0.0562\n",
      "Epoch [67/100], Step [300/391], Loss: 0.0564\n",
      "Epoch 67 finished. Loss: 0.06317599848522555\n",
      "Epoch [68/100], Step [100/391], Loss: 0.0560\n",
      "Epoch [68/100], Step [200/391], Loss: 0.0667\n",
      "Epoch [68/100], Step [300/391], Loss: 0.0762\n",
      "Epoch 68 finished. Loss: 0.07202923220827642\n",
      "Epoch [69/100], Step [100/391], Loss: 0.0576\n",
      "Epoch [69/100], Step [200/391], Loss: 0.0531\n",
      "Epoch [69/100], Step [300/391], Loss: 0.0546\n",
      "Epoch 69 finished. Loss: 0.05636757902581902\n",
      "Epoch [70/100], Step [100/391], Loss: 0.0510\n",
      "Epoch [70/100], Step [200/391], Loss: 0.0484\n",
      "Epoch [70/100], Step [300/391], Loss: 0.0504\n",
      "Epoch 70 finished. Loss: 0.050290074505512136\n",
      "Epoch [71/100], Step [100/391], Loss: 0.0379\n",
      "Epoch [71/100], Step [200/391], Loss: 0.0363\n",
      "Epoch [71/100], Step [300/391], Loss: 0.0500\n",
      "Epoch 71 finished. Loss: 0.04891660023192444\n",
      "Epoch [72/100], Step [100/391], Loss: 0.0628\n",
      "Epoch [72/100], Step [200/391], Loss: 0.0771\n",
      "Epoch [72/100], Step [300/391], Loss: 0.0651\n",
      "Epoch 72 finished. Loss: 0.07171190757056713\n",
      "Epoch [73/100], Step [100/391], Loss: 0.0732\n",
      "Epoch [73/100], Step [200/391], Loss: 0.0778\n",
      "Epoch [73/100], Step [300/391], Loss: 0.0698\n",
      "Epoch 73 finished. Loss: 0.06913633376855374\n",
      "Epoch [74/100], Step [100/391], Loss: 0.0473\n",
      "Epoch [74/100], Step [200/391], Loss: 0.0579\n",
      "Epoch [74/100], Step [300/391], Loss: 0.0580\n",
      "Epoch 74 finished. Loss: 0.056699508390820504\n",
      "Epoch [75/100], Step [100/391], Loss: 0.0530\n",
      "Epoch [75/100], Step [200/391], Loss: 0.0646\n",
      "Epoch [75/100], Step [300/391], Loss: 0.0626\n",
      "Epoch 75 finished. Loss: 0.059917485553418734\n",
      "Epoch [76/100], Step [100/391], Loss: 0.0612\n",
      "Epoch [76/100], Step [200/391], Loss: 0.0617\n",
      "Epoch [76/100], Step [300/391], Loss: 0.0650\n",
      "Epoch 76 finished. Loss: 0.06657590076346379\n",
      "Epoch [77/100], Step [100/391], Loss: 0.0713\n",
      "Epoch [77/100], Step [200/391], Loss: 0.0756\n",
      "Epoch [77/100], Step [300/391], Loss: 0.0793\n",
      "Epoch 77 finished. Loss: 0.07593932621362036\n",
      "Epoch [78/100], Step [100/391], Loss: 0.0511\n",
      "Epoch [78/100], Step [200/391], Loss: 0.0510\n",
      "Epoch [78/100], Step [300/391], Loss: 0.0487\n",
      "Epoch 78 finished. Loss: 0.049736154328226624\n",
      "Epoch [79/100], Step [100/391], Loss: 0.0356\n",
      "Epoch [79/100], Step [200/391], Loss: 0.0386\n",
      "Epoch [79/100], Step [300/391], Loss: 0.0420\n",
      "Epoch 79 finished. Loss: 0.04399107815519147\n",
      "Epoch [80/100], Step [100/391], Loss: 0.0598\n",
      "Epoch [80/100], Step [200/391], Loss: 0.0512\n",
      "Epoch [80/100], Step [300/391], Loss: 0.0475\n",
      "Epoch 80 finished. Loss: 0.054629434244302305\n",
      "Epoch [81/100], Step [100/391], Loss: 0.0561\n",
      "Epoch [81/100], Step [200/391], Loss: 0.0527\n",
      "Epoch [81/100], Step [300/391], Loss: 0.0494\n",
      "Epoch 81 finished. Loss: 0.05261320238837691\n",
      "Epoch [82/100], Step [100/391], Loss: 0.0454\n",
      "Epoch [82/100], Step [200/391], Loss: 0.0531\n",
      "Epoch [82/100], Step [300/391], Loss: 0.0584\n",
      "Epoch 82 finished. Loss: 0.055775894311821214\n",
      "Epoch [83/100], Step [100/391], Loss: 0.0528\n",
      "Epoch [83/100], Step [200/391], Loss: 0.0685\n",
      "Epoch [83/100], Step [300/391], Loss: 0.0647\n",
      "Epoch 83 finished. Loss: 0.06679095618684044\n",
      "Epoch [84/100], Step [100/391], Loss: 0.0500\n",
      "Epoch [84/100], Step [200/391], Loss: 0.0493\n",
      "Epoch [84/100], Step [300/391], Loss: 0.0457\n",
      "Epoch 84 finished. Loss: 0.04794178111002306\n",
      "Epoch [85/100], Step [100/391], Loss: 0.0358\n",
      "Epoch [85/100], Step [200/391], Loss: 0.0375\n",
      "Epoch [85/100], Step [300/391], Loss: 0.0439\n",
      "Epoch 85 finished. Loss: 0.04131105167271994\n",
      "Epoch [86/100], Step [100/391], Loss: 0.0380\n",
      "Epoch [86/100], Step [200/391], Loss: 0.0377\n",
      "Epoch [86/100], Step [300/391], Loss: 0.0489\n",
      "Epoch 86 finished. Loss: 0.0460716624401243\n",
      "Epoch [87/100], Step [100/391], Loss: 0.0712\n",
      "Epoch [87/100], Step [200/391], Loss: 0.0662\n",
      "Epoch [87/100], Step [300/391], Loss: 0.0591\n",
      "Epoch 87 finished. Loss: 0.06305592595254217\n",
      "Epoch [88/100], Step [100/391], Loss: 0.0485\n",
      "Epoch [88/100], Step [200/391], Loss: 0.0565\n",
      "Epoch [88/100], Step [300/391], Loss: 0.0639\n",
      "Epoch 88 finished. Loss: 0.06040185351815561\n",
      "Epoch [89/100], Step [100/391], Loss: 0.0432\n",
      "Epoch [89/100], Step [200/391], Loss: 0.0501\n",
      "Epoch [89/100], Step [300/391], Loss: 0.0652\n",
      "Epoch 89 finished. Loss: 0.05298145699417195\n",
      "Epoch [90/100], Step [100/391], Loss: 0.0407\n",
      "Epoch [90/100], Step [200/391], Loss: 0.0428\n",
      "Epoch [90/100], Step [300/391], Loss: 0.0451\n",
      "Epoch 90 finished. Loss: 0.046346145002004666\n",
      "Epoch [91/100], Step [100/391], Loss: 0.0445\n",
      "Epoch [91/100], Step [200/391], Loss: 0.0498\n",
      "Epoch [91/100], Step [300/391], Loss: 0.0613\n",
      "Epoch 91 finished. Loss: 0.05355171879034137\n",
      "Epoch [92/100], Step [100/391], Loss: 0.0626\n",
      "Epoch [92/100], Step [200/391], Loss: 0.0571\n",
      "Epoch [92/100], Step [300/391], Loss: 0.0535\n",
      "Epoch 92 finished. Loss: 0.058224296325917746\n",
      "Epoch [93/100], Step [100/391], Loss: 0.0428\n",
      "Epoch [93/100], Step [200/391], Loss: 0.0380\n",
      "Epoch [93/100], Step [300/391], Loss: 0.0428\n",
      "Epoch 93 finished. Loss: 0.0424156276952199\n",
      "Epoch [94/100], Step [100/391], Loss: 0.0348\n",
      "Epoch [94/100], Step [200/391], Loss: 0.0399\n",
      "Epoch [94/100], Step [300/391], Loss: 0.0545\n",
      "Epoch 94 finished. Loss: 0.044954655750337844\n",
      "Epoch [95/100], Step [100/391], Loss: 0.0432\n",
      "Epoch [95/100], Step [200/391], Loss: 0.0462\n",
      "Epoch [95/100], Step [300/391], Loss: 0.0498\n",
      "Epoch 95 finished. Loss: 0.05069033666025571\n",
      "Epoch [96/100], Step [100/391], Loss: 0.0423\n",
      "Epoch [96/100], Step [200/391], Loss: 0.0367\n",
      "Epoch [96/100], Step [300/391], Loss: 0.0469\n",
      "Epoch 96 finished. Loss: 0.04357585157898953\n",
      "Epoch [97/100], Step [100/391], Loss: 0.0350\n",
      "Epoch [97/100], Step [200/391], Loss: 0.0317\n",
      "Epoch [97/100], Step [300/391], Loss: 0.0303\n",
      "Epoch 97 finished. Loss: 0.03524783503024808\n",
      "Epoch [98/100], Step [100/391], Loss: 0.0468\n",
      "Epoch [98/100], Step [200/391], Loss: 0.0530\n",
      "Epoch [98/100], Step [300/391], Loss: 0.0557\n",
      "Epoch 98 finished. Loss: 0.05153445151629751\n",
      "Epoch [99/100], Step [100/391], Loss: 0.0536\n",
      "Epoch [99/100], Step [200/391], Loss: 0.0515\n",
      "Epoch [99/100], Step [300/391], Loss: 0.0501\n",
      "Epoch 99 finished. Loss: 0.05508982689272317\n",
      "Epoch [100/100], Step [100/391], Loss: 0.0614\n",
      "Epoch [100/100], Step [200/391], Loss: 0.0458\n",
      "Epoch [100/100], Step [300/391], Loss: 0.0379\n",
      "Epoch 100 finished. Loss: 0.04586442735027093\n"
     ]
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "accumulation_steps = 2  # Simulates batch_size = 256 with batch_size = 128\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    epoch_loss = 0.0\n",
    "\n",
    "    optimizer.zero_grad()  # Ensure gradients are zeroed before accumulation\n",
    "    for i, (inputs, labels) in enumerate(training_loader):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Normalize loss for accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights every `accumulation_steps` iterations\n",
    "        if (i + 1) % accumulation_steps == 0 or (i + 1) == len(training_loader):\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()  # Reset gradients after step\n",
    "\n",
    "        running_loss += loss.item() * accumulation_steps  # Re-scale loss back\n",
    "        epoch_loss += loss.item() * accumulation_steps  # Re-scale loss back\n",
    "\n",
    "        if (i + 1) % __n == 0:  # For monitoring intermediate steps\n",
    "            print(f\"Epoch [{epoch + 1}/{epochs}], Step [{i + 1}/{len(training_loader)}], \"\n",
    "                  f\"Loss: {running_loss / __n:.4f}\")\n",
    "            running_loss = 0.0\n",
    "\n",
    "\n",
    "    epoch_loss = epoch_loss / len(training_loader)\n",
    "    print(f\"Epoch {epoch + 1} finished. Loss: {epoch_loss}\")\n",
    "    _training_losses.append(epoch_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732013514207,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "DZtxu_vtTSuI"
   },
   "outputs": [],
   "source": [
    "_end = time.time()\n",
    "torch.save(model.state_dict(), os.path.join(CHECKPOINT_PATH, f'final_checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1732013514207,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "GNGyK4LmfrPI",
    "outputId": "19f3e226-f0b8-4157-b475-0f6026370c37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1732013514.0293493\n"
     ]
    }
   ],
   "source": [
    "print(_end)\n",
    "\n",
    "with open(os.path.join(CHECKPOINT_PATH, 'trainning_time.txt'), 'w') as f:\n",
    "  f.write(f\"Start Time: {_start}\\nEnd Time:{_end}\\nTotal Time:{_end - _start}\")\n",
    "\n",
    "\n",
    "DF = pd.DataFrame(np.array(_training_losses))\n",
    "DF.to_csv(os.path.join(CHECKPOINT_PATH, 'losses.csv'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 119,
     "status": "ok",
     "timestamp": 1732013514324,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "eWOCOV2428mZ",
    "outputId": "6fd06005-5d5e-4494-8655-457568abdf5e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "====================================================================================================\n",
       "Layer (type:depth-idx)                             Output Shape              Param #\n",
       "====================================================================================================\n",
       "Vim                                                [128, 10]                 96\n",
       "├─Sequential: 1-1                                  [128, 64, 96]             --\n",
       "│    └─Rearrange: 2-1                              [128, 64, 48]             --\n",
       "│    └─Linear: 2-2                                 [128, 64, 96]             4,704\n",
       "├─Dropout: 1-2                                     [128, 64, 96]             --\n",
       "├─ModuleList: 1-3                                  --                        --\n",
       "│    └─VisionEncoderMambaBlock: 2-3                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-1                         [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-2                            [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-3                              [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-4                            [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-5         [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-6                          [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-7                               [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-8         [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-9                          [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-10                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-4                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-11                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-12                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-13                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-14                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-15        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-16                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-17                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-18        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-19                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-20                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-5                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-21                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-22                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-23                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-24                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-25        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-26                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-27                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-28        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-29                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-30                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-6                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-31                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-32                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-33                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-34                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-35        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-36                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-37                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-38        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-39                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-40                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-7                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-41                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-42                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-43                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-44                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-45        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-46                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-47                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-48        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-49                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-50                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-8                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-51                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-52                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-53                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-54                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-55        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-56                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-57                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-58        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-59                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-60                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-9                [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-61                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-62                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-63                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-64                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-65        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-66                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-67                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-68        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-69                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-70                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-10               [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-71                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-72                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-73                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-74                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-75        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-76                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-77                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-78        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-79                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-80                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-11               [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-81                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-82                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-83                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-84                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-85        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-86                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-87                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-88        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-89                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-90                              [128, 64, 96]             (recursive)\n",
       "│    └─VisionEncoderMambaBlock: 2-12               [128, 64, 96]             --\n",
       "│    │    └─LayerNorm: 3-91                        [128, 64, 96]             192\n",
       "│    │    └─Linear: 3-92                           [128, 64, 96]             9,312\n",
       "│    │    └─SiLU: 3-93                             [128, 64, 96]             --\n",
       "│    │    └─Linear: 3-94                           [128, 64, 96]             (recursive)\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-95        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-96                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-97                              [128, 64, 96]             30,912\n",
       "│    │    └─MobileNetV2_DepthwiseConv: 3-98        [128, 96, 64]             76,608\n",
       "│    │    └─Softplus: 3-99                         [128, 96, 64]             --\n",
       "│    │    └─SSM: 3-100                             [128, 64, 96]             (recursive)\n",
       "├─Identity: 1-4                                    [128, 64, 96]             --\n",
       "├─Sequential: 1-5                                  [128, 10]                 --\n",
       "│    └─Reduce: 2-13                                [128, 96]                 --\n",
       "│    └─LayerNorm: 2-14                             [128, 96]                 192\n",
       "│    └─Linear: 2-15                                [128, 10]                 970\n",
       "====================================================================================================\n",
       "Total params: 1,942,282\n",
       "Trainable params: 1,942,282\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 12.35\n",
       "====================================================================================================\n",
       "Input size (MB): 1.57\n",
       "Forward/backward pass size (MB): 2858.53\n",
       "Params size (MB): 7.40\n",
       "Estimated Total Size (MB): 2867.50\n",
       "===================================================================================================="
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(batch_size, 3, 32, 32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11408,
     "status": "ok",
     "timestamp": 1732013525732,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "Qhg5IBirMhdO",
    "outputId": "886257cd-e26f-48f1-c415-b79e4134067c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy of the model test images: 68.27%\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs,labels in testing_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "print(f\"accuracy of the model test images: {(100 * correct / total):.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1732013525732,
     "user": {
      "displayName": "Lucas Martinez",
      "userId": "17839641396214676141"
     },
     "user_tz": 300
    },
    "id": "jeE1IeuDNjro",
    "outputId": "076eb3dd-aaee-4b6e-e336-509ceefd52a3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  41346 KiB |  20916 MiB |   5628 TiB |   5628 TiB |\n",
      "|       from large pool |  18176 KiB |  20883 MiB |   5626 TiB |   5626 TiB |\n",
      "|       from small pool |  23170 KiB |     38 MiB |      1 TiB |      1 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  41346 KiB |  20916 MiB |   5628 TiB |   5628 TiB |\n",
      "|       from large pool |  18176 KiB |  20883 MiB |   5626 TiB |   5626 TiB |\n",
      "|       from small pool |  23170 KiB |     38 MiB |      1 TiB |      1 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Requested memory      |  41272 KiB |  20914 MiB |   5626 TiB |   5626 TiB |\n",
      "|       from large pool |  18176 KiB |  20882 MiB |   5625 TiB |   5625 TiB |\n",
      "|       from small pool |  23096 KiB |     38 MiB |      1 TiB |      1 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |  21192 MiB |  21192 MiB |  21192 MiB |      0 B   |\n",
      "|       from large pool |  21152 MiB |  21152 MiB |  21152 MiB |      0 B   |\n",
      "|       from small pool |     40 MiB |     40 MiB |     40 MiB |      0 B   |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory | 159358 KiB |   5931 MiB |    804 TiB |    804 TiB |\n",
      "|       from large pool | 149760 KiB |   5925 MiB |    802 TiB |    802 TiB |\n",
      "|       from small pool |   9598 KiB |     11 MiB |      2 TiB |      2 TiB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |    1022    |    1879    |  111594 K  |  111593 K  |\n",
      "|       from large pool |       3    |     402    |   77003 K  |   77003 K  |\n",
      "|       from small pool |    1019    |    1568    |   34591 K  |   34590 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |    1022    |    1879    |  111594 K  |  111593 K  |\n",
      "|       from large pool |       3    |     402    |   77003 K  |   77003 K  |\n",
      "|       from small pool |    1019    |    1568    |   34591 K  |   34590 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |     110    |     110    |     110    |       0    |\n",
      "|       from large pool |      90    |      90    |      90    |       0    |\n",
      "|       from small pool |      20    |      20    |      20    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      21    |      69    |   37110 K  |   37110 K  |\n",
      "|       from large pool |       4    |      60    |   24143 K  |   24143 K  |\n",
      "|       from small pool |      17    |      43    |   12966 K  |   12966 K  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize allocations  |       0    |       0    |       0    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Oversize GPU segments |       0    |       0    |       0    |       0    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
